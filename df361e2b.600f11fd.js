(window.webpackJsonp=window.webpackJsonp||[]).push([[27],{102:function(e,t,n){"use strict";n.d(t,"a",(function(){return u})),n.d(t,"b",(function(){return f}));var i=n(0),o=n.n(i);function a(e,t,n){return t in e?Object.defineProperty(e,t,{value:n,enumerable:!0,configurable:!0,writable:!0}):e[t]=n,e}function r(e,t){var n=Object.keys(e);if(Object.getOwnPropertySymbols){var i=Object.getOwnPropertySymbols(e);t&&(i=i.filter((function(t){return Object.getOwnPropertyDescriptor(e,t).enumerable}))),n.push.apply(n,i)}return n}function c(e){for(var t=1;t<arguments.length;t++){var n=null!=arguments[t]?arguments[t]:{};t%2?r(Object(n),!0).forEach((function(t){a(e,t,n[t])})):Object.getOwnPropertyDescriptors?Object.defineProperties(e,Object.getOwnPropertyDescriptors(n)):r(Object(n)).forEach((function(t){Object.defineProperty(e,t,Object.getOwnPropertyDescriptor(n,t))}))}return e}function l(e,t){if(null==e)return{};var n,i,o=function(e,t){if(null==e)return{};var n,i,o={},a=Object.keys(e);for(i=0;i<a.length;i++)n=a[i],t.indexOf(n)>=0||(o[n]=e[n]);return o}(e,t);if(Object.getOwnPropertySymbols){var a=Object.getOwnPropertySymbols(e);for(i=0;i<a.length;i++)n=a[i],t.indexOf(n)>=0||Object.prototype.propertyIsEnumerable.call(e,n)&&(o[n]=e[n])}return o}var s=o.a.createContext({}),d=function(e){var t=o.a.useContext(s),n=t;return e&&(n="function"==typeof e?e(t):c(c({},t),e)),n},u=function(e){var t=d(e.components);return o.a.createElement(s.Provider,{value:t},e.children)},p={inlineCode:"code",wrapper:function(e){var t=e.children;return o.a.createElement(o.a.Fragment,{},t)}},h=o.a.forwardRef((function(e,t){var n=e.components,i=e.mdxType,a=e.originalType,r=e.parentName,s=l(e,["components","mdxType","originalType","parentName"]),u=d(n),h=i,f=u["".concat(r,".").concat(h)]||u[h]||p[h]||a;return n?o.a.createElement(f,c(c({ref:t},s),{},{components:n})):o.a.createElement(f,c({ref:t},s))}));function f(e,t){var n=arguments,i=t&&t.mdxType;if("string"==typeof e||i){var a=n.length,r=new Array(a);r[0]=h;var c={};for(var l in t)hasOwnProperty.call(t,l)&&(c[l]=t[l]);c.originalType=e,c.mdxType="string"==typeof e?e:i,r[1]=c;for(var s=2;s<a;s++)r[s]=n[s];return o.a.createElement.apply(null,r)}return o.a.createElement.apply(null,n)}h.displayName="MDXCreateElement"},157:function(e,t,n){"use strict";n.r(t),t.default=n.p+"assets/images/intro-47f913b212891442343e483f5ca2ec47.png"},94:function(e,t,n){"use strict";n.r(t),n.d(t,"frontMatter",(function(){return r})),n.d(t,"metadata",(function(){return c})),n.d(t,"rightToc",(function(){return l})),n.d(t,"default",(function(){return d}));var i=n(3),o=n(7),a=(n(0),n(102)),r={id:"doc2",title:"Introduction & Motivation",sidebar_label:"Introduction",custom_edit_url:null},c={unversionedId:"doc2",id:"doc2",isDocsHomePage:!1,title:"Introduction & Motivation",description:"Human activity recognition can provide context on the individual\u2019s lifestyle and aid in providing valuable information to the person on their daily intake of food, physical activity, and other required information. It becomes particularly useful for eldercare and health care as a context based assistive technology,  in addition to this, recognizing the activity can help systems to generate personalized recommendations for a user and enrich their experience. Sensor based activity recognition is prevalent and mobile devices like smartphones are one among the ubiquitous technology to monitor the user\u2019s lifestyle. It is estimated that there are 5.23 billion people that have a mobile device in the world amounting to more than 66.79% of the world's population.",source:"@site/docs/doc2.md",slug:"/doc2",permalink:"/ASR/docs/doc2",editUrl:null,version:"current",sidebar_label:"Introduction",sidebar:"someSidebar",previous:{title:"Introduction",permalink:"/ASR/docs/"},next:{title:"Related Work & Contributions",permalink:"/ASR/docs/doc3"}},l=[],s={rightToc:l};function d(e){var t=e.components,r=Object(o.a)(e,["components"]);return Object(a.b)("wrapper",Object(i.a)({},s,r,{components:t,mdxType:"MDXLayout"}),Object(a.b)("p",null,"Human activity recognition can provide context on the individual\u2019s lifestyle and aid in providing valuable information to the person on their daily intake of food, physical activity, and other required information. It becomes particularly useful for eldercare and health care as a context based assistive technology,  in addition to this, recognizing the activity can help systems to generate personalized recommendations for a user and enrich their experience. Sensor based activity recognition is prevalent and mobile devices like smartphones are one among the ubiquitous technology to monitor the user\u2019s lifestyle. It is estimated that there are 5.23 billion people that have a mobile device in the world amounting to more than 66.79% of the world's population.\nSmartphones have built-in accelerometers which make it capable of recognizing daily activities. This helps to gather an understanding of what people are doing at any moment in time, anticipating what they would probably do next, and providing services automatically and according to the context."),Object(a.b)("p",null,Object(a.b)("img",{alt:"img",src:n(157).default})),Object(a.b)("p",null,"There are a couple of challenges that limit the translation of this idea into a product, like Intraclass Variability, Interclass Similarity, Class Imbalance, Ground Truth Annotation.In this project we specifically focus on two major challenges in building an activity recognition model, "),Object(a.b)("p",null,"First we address the problem of Intraclass variability which implies that same activity can be performed differently by different individuals and even differently by the same individual over time. That means the classifier may have poor performance when it is applied to the test samples gathered from the people different from the training set. One solution to this problem is to collect the necessary data from the individuals beforehand and this could be time-consuming and infeasible."),Object(a.b)("p",null,"Second we address the problem of sensor placement variations, i.e an activity aware product should be able to recognize the individual\u2019s activity with variations in the placement of sensors as well, since mobile phones can be freely carried around instead of getting strapped to a part of the body like a smart watch, it becomes increasingly difficult to build a recognizer that can account for this variation. Hence an activity recognition method should function independent of the phone's position along the subject's body and should be capable of providing high recognition results even in the absence of adequate amount of training data from different positions."),Object(a.b)("p",null,"We leverage active and semi-supervised learning methods to solve for the above two problems, first by building a personalized model for an individual, by continually learning from the activities of the individual over time, and second to account for sensor placement variations, we adapt a model trained on data collected from a individual in a particular sensor placement and adapt it to recognize activities when the placement changes. "))}d.isMDXComponent=!0}}]);